{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting Up the environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install PyPDF2\n",
    "%pip install pandas\n",
    "%pip install re\n",
    "%pip install openai==0.28\n",
    "%pip install openpyxl\n",
    "%pip install pdf2image pytesseract\n",
    "%pip install PyMuPDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "import glob\n",
    "from PyPDF2 import PdfReader\n",
    "import pandas as pd\n",
    "import re\n",
    "from pdf2image import convert_from_path\n",
    "import pytesseract\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "import fitz\n",
    "import json\n",
    "from openpyxl import Workbook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load OpenAI API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "api_key = os.getenv('OPENAI_API_KEY')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Cleaning: Extract Text from the PDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_text_extracted_meaningful(text):\n",
    "    # Check if the text is too short or seems not meaningful\n",
    "    return len(text.strip()) > 100  # Adjust the threshold as needed\n",
    "\n",
    "# Function to extract text from PDF\n",
    "def extract_text_from_pdf(pdf_path):\n",
    "    try:\n",
    "        # Try extracting text using PyMuPDF\n",
    "        text = \"\"\n",
    "        document = fitz.open(pdf_path)\n",
    "        for page_num in range(len(document)):\n",
    "            page = document.load_page(page_num)\n",
    "            extracted_text = page.get_text(\"text\")\n",
    "            if extracted_text:\n",
    "                text += extracted_text\n",
    "\n",
    "        # Fallback to OCR if text extraction is not meaningful\n",
    "        if not is_text_extracted_meaningful(text):\n",
    "            text = \"\"\n",
    "            images = convert_from_path(pdf_path)\n",
    "            for image in images:\n",
    "                text += pytesseract.image_to_string(image)\n",
    "        \n",
    "        text = text.replace(\"\\n\", \" \").replace(\"\\t\", \" \")\n",
    "        return text\n",
    "    except Exception as e:\n",
    "        print(f\"Error occurred: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GPT 4o"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Personality Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "personality_persona = \"You are an expert in analyzing student application for a universty.You have been asked to review a student's application essay and provide an analysis on the student's motivation.\"\n",
    "personality_prompt = f\"\"\"\n",
    "Please analyze the following student's application statement to identify 10 keywords that indicate the student's personality. Provide an explanation of the student personality based on the keyword. For each keyword, provide the following information in the specified format:\n",
    "\n",
    "[\n",
    "  {{\n",
    "    \"Keyword\": \"\",\n",
    "    \"Explanation\": \"\",\n",
    "    \"Sentence Related to Keyword\": \"\"\n",
    "  }}\n",
    "]\n",
    "\n",
    "The output should be in this JSON format and no other output is accepted.\n",
    "\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Motivation Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gpt_input(statement,persona,prompt):\n",
    "    response = openai.ChatCompletion.create(\n",
    "        model=\"gpt-4o\",  # Use the appropriate model name\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\":f\"{persona} {prompt}\"},\n",
    "            {\"role\": \"user\", \"content\": statement},\n",
    "        ]\n",
    "    )\n",
    "\n",
    " # Extract the assistant's reply\n",
    "    assistant_reply = response['choices'][0]['message']['content']\n",
    "    return assistant_reply"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parsing the Output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Personality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_personality_text(analyzed_text):\n",
    "    lines = analyzed_text.split('\\n')\n",
    "    parsed_data = []\n",
    "    current_keyword = \"\"\n",
    "    current_explanation = \"\"\n",
    "    \n",
    "    # Regular expression patterns to match different keyword formats\n",
    "    pattern1 = re.compile(r'\\d+\\.\\s*\\*\\*(.*?)\\*\\*')\n",
    "    pattern2 = re.compile(r'\\*\\*\\d+\\.\\s*(.*?):\\*\\*')\n",
    "    \n",
    "    for line in lines:\n",
    "        match1 = pattern1.match(line)\n",
    "        match2 = pattern2.match(line)\n",
    "        if match1:\n",
    "            if current_keyword and current_explanation:\n",
    "                parsed_data.append([current_keyword, current_explanation.strip()])\n",
    "            current_keyword = match1.group(1).strip()\n",
    "            current_explanation = line[match1.end():].strip()  # Start explanation from the rest of the line if present\n",
    "        elif match2:\n",
    "            if current_keyword and current_explanation:\n",
    "                parsed_data.append([current_keyword, current_explanation.strip()])\n",
    "            current_keyword = match2.group(1).strip()\n",
    "            current_explanation = line[match2.end():].strip()  # Start explanation from the rest of the line if present\n",
    "        else:\n",
    "            if current_keyword:\n",
    "                if line.strip():  # Only add lines that are not just empty or whitespace\n",
    "                    current_explanation += ' ' + line.strip()\n",
    "    \n",
    "    # Add the last keyword and explanation if present\n",
    "    if current_keyword and current_explanation:\n",
    "        parsed_data.append([current_keyword, current_explanation.strip()])\n",
    "    \n",
    "    return parsed_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Motivation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def parse_motivation_text(analyzed_text):\n",
    "#     if not analyzed_text or not analyzed_text.strip():\n",
    "#             print(\"The input text is empty or None.\")\n",
    "#             return None\n",
    "        \n",
    "#     try:\n",
    "#         data = json.loads(analyzed_text)\n",
    "#     except json.JSONDecodeError as e:\n",
    "#         print(f\"JSON decoding error: {e}\")\n",
    "#         return None\n",
    "    \n",
    "#     return data\n",
    "\n",
    "def parse_motivation_text(analyzed_text):\n",
    "    # Check if the input text is empty or None\n",
    "    if not analyzed_text or not analyzed_text.strip():\n",
    "        print(\"The input text is empty or None.\")\n",
    "        return None\n",
    "    \n",
    "    # Check for and remove the JSON heading if present\n",
    "    if analyzed_text.startswith(\"```json\"):\n",
    "        analyzed_text = analyzed_text.split('\\n', 1)[1]\n",
    "    \n",
    "    try:\n",
    "        # Attempt to decode the JSON\n",
    "        data = json.loads(analyzed_text)\n",
    "    except json.JSONDecodeError as e:\n",
    "        print(f\"JSON decoding error: {e}\")\n",
    "        return None\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save Output to Excel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_to_excel(parsed_data, ID, output_file):\n",
    "    # # Create a DataFrame from the parsed data\n",
    "    # df = pd.DataFrame(parsed_data, columns=['Personality', 'Explanation'])\n",
    "    \n",
    "    # # Add a column for the file name\n",
    "    # df.insert(0, 'File Name', ID)\n",
    "    \n",
    "    # if os.path.exists(output_file):\n",
    "    #     # If the file exists, read the existing data\n",
    "    #     existing_df = pd.read_excel(output_file)\n",
    "    #     # Append the new data\n",
    "    #     df = pd.concat([existing_df, df], ignore_index=True)\n",
    "    \n",
    "    # # Save the updated DataFrame back to the Excel file\n",
    "    # df.to_excel(output_file, index=False)\n",
    "    # print(f\"Results have been saved to {output_file}\")\n",
    "     # Create a DataFrame from the parsed data\n",
    "    # Check if parsed_data is valid\n",
    "    if parsed_data is None:\n",
    "        print(\"No data to save.\")\n",
    "        return\n",
    "    \n",
    "    # Create a DataFrame from the parsed data\n",
    "    df = pd.DataFrame(parsed_data, columns=['Keyword', 'Explanation', 'Sentence Related to Keyword'])\n",
    "    \n",
    "    # Add a column for the file name\n",
    "    df.insert(0, 'File Name', ID)\n",
    "    \n",
    "    if os.path.exists(output_file):\n",
    "        # If the file exists, read the existing data\n",
    "        existing_df = pd.read_excel(output_file)\n",
    "        # Append the new data\n",
    "        df = pd.concat([existing_df, df], ignore_index=True)\n",
    "    \n",
    "    # Save the updated DataFrame back to the Excel file\n",
    "    df.to_excel(output_file, index=False, engine='openpyxl')\n",
    "    print(f\"Results have been saved to {output_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Main Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "motivation_persona = \"You are an expert in analyzing student applications for a university. You have been asked to review a student's application essay and provide an analysis on the student's motivation.\"\n",
    "motivation_prompt = f\"\"\"\n",
    "Please analyze the following student's application statement to identify 10 keywords that indicate their motivation to study in university. For each keyword, provide the following information in the specified format:\n",
    "[\n",
    "  {{\n",
    "    \"Keyword\": \"\",\n",
    "    \"Explanation\": \"\",\n",
    "    \"Sentence Related to Keyword\": \"\"\n",
    "  }}\n",
    "]\n",
    "The output should be in this format and no other output is accepted.\n",
    "\"\"\"\n",
    "# personality_persona = \"You are an expert in analyzing student application for a universty.You have been asked to review a student's application essay and provide an analysis on the student's motivation.\"\n",
    "# personality_prompt = f\"\"\"\n",
    "# Please analyze the following student's application statement to identify 10 keywords that indicate the student's personality. Provide an explanation of the student personality based on the keyword. For each keyword, provide the following information in the specified format:\n",
    "\n",
    "# [\n",
    "#   {{\n",
    "#     \"Keyword\": \"\",\n",
    "#     \"Explanation\": \"\",\n",
    "#     \"Sentence Related to Keyword\": \"\"\n",
    "#   }}\n",
    "# ]\n",
    "\n",
    "# The output should be in this format and no other output is accepted.\n",
    "\n",
    "# \"\"\"\n",
    "\n",
    "base_dir = 'temp'\n",
    "sub_dirs = ['c-enrolled']\n",
    "output_file = 'temp_22c.xlsx'\n",
    "\n",
    "for sub_dir in sub_dirs:\n",
    "    sub_dir_path = os.path.join(base_dir, sub_dir)\n",
    "    print(f\"Processing subdirectory: {sub_dir_path}\")\n",
    "    \n",
    "    student_dirs = sorted(glob.glob(os.path.join(sub_dir_path, '2021-A-*')))\n",
    "    \n",
    "    if not student_dirs:\n",
    "        print(f\"No student directories found in {sub_dir_path}\\n\")\n",
    "        continue\n",
    "        \n",
    "    for student_dir in student_dirs:\n",
    "        print(f\"Processing student directory: {student_dir}\\n\")\n",
    "        \n",
    "        pdf_files = sorted(glob.glob(os.path.join(student_dir, '*.pdf')))\n",
    "        if not pdf_files:\n",
    "            print(f\"No PDF files found in {student_dir}\")\n",
    "            continue\n",
    "        \n",
    "        concatenated_text = \"\"\n",
    "        for pdf_file in pdf_files:\n",
    "            print(f\"Reading PDF file: {pdf_file}\")\n",
    "            pdf_text = extract_text_from_pdf(pdf_file)\n",
    "            concatenated_text += pdf_text + \" \"\n",
    "        \n",
    "        print(f\"Concatenated text for {student_dir}:\\n{concatenated_text[:1000]}...\")  \n",
    "        print(f\"Total length of concatenated text: {len(concatenated_text)}\")\n",
    "        \n",
    "        analyzed_text = gpt_input(concatenated_text, motivation_persona, motivation_prompt)\n",
    "        print(f\"Analyzed text for {student_dir}:\\n{analyzed_text}\\n\")\n",
    "        parsed_data = parse_motivation_text(analyzed_text)\n",
    "        print(f\"Parsed data for {student_dir}:\\n{parsed_data}\\n\")\n",
    "        student_dir_name = os.path.basename(student_dir)\n",
    "        save_to_excel(parsed_data, student_dir_name, output_file)\n",
    "\n",
    "print(f\"Results have been saved to {output_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Motivation Category Analysis by GPT API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = 'Statistics_2022eecs_GPT.xlsx'\n",
    "\n",
    "df = pd.read_excel(file_path, sheet_name='ENROLLED_MOTIVATION')  # Replace 'Sheet1' with your sheet name\n",
    "# Extract unique values from a specific column (e.g., 'Column1')\n",
    "unique_values = df['Keyword'].unique()\n",
    "\n",
    "# Create a new DataFrame for the unique values\n",
    "unique_df = pd.DataFrame(unique_values, columns=['Unique Values'])\n",
    "unique_list = unique_df.values.flatten().tolist()\n",
    "print(unique_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gave ChatGPT Persona"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "persona = \"\"\"\n",
    "You are a data analysis specializing in natural language processing and keyword extraction.\n",
    "You will be given a list of keywords about student motivation to study in university, reorganize the following keywords into appropriate categories.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gpt_personality_conclusion(statement,persona):\n",
    "    response = openai.ChatCompletion.create(\n",
    "        model=\"gpt-4o\",  # Use the appropriate model name\n",
    "        messages=[\n",
    "            {\n",
    "                \"role\": \"system\", \n",
    "                \"content\": persona\n",
    "            },\n",
    "            {\"role\": \"user\", \"content\": statement},\n",
    "        ],\n",
    "    )\n",
    "\n",
    "    # Extract the assistant's reply\n",
    "    assistant_reply = response['choices'][0]['message']['content']\n",
    "    return assistant_reply"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Find the top keywords from all the keywords in excel\n",
    "keywords_list = unique_list\n",
    "\n",
    "keywords_prompt = f\"\"\"{persona}\\n. Here is your task:\\n\n",
    "                    1. Read all of the Keywords.\\n\n",
    "                    2. Take the keyword input and recategorized this keyword into 20 category.\\n\n",
    "                    2. If they mention school name such as NTHU, make the category School.\\n\n",
    "                    3. Make sure all keyword got into category.\\n\n",
    "                    4. Make the category more specific. No miscellaneous category.\\n\n",
    "                    5. You should output the data in the following format:\\n\\n\n",
    "                        1. [Category]: Keyword, Keyword, Keyword,..\\n\n",
    "                    The output should be in this format and no other output is accepted.\\n\n",
    "                    Here is the list of keywords that shows student motivation to study in university: {keywords_list}\\n\n",
    "                    \"\"\"\n",
    "keywords_category = gpt_personality_conclusion(keywords_prompt,persona)\n",
    "print(\"Category and Keyword:\")\n",
    "print(keywords_category)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use regular expressions to extract keyword categories and keywords\n",
    "pattern = re.compile(r'\\d+\\.\\s+([^:]+):\\s+(.+)')\n",
    "matches = pattern.findall(keywords_category)\n",
    "\n",
    "# Prepare data for the DataFrame\n",
    "data = []\n",
    "for category, keywords in matches:\n",
    "    keywords_list = [k.strip() for k in keywords.split(',')]\n",
    "    for keyword in keywords_list:\n",
    "        data.append({'keyword category': category.strip(), 'keyword': keyword})\n",
    "\n",
    "# Convert the data to a DataFrame\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Save the DataFrame to an Excel file\n",
    "df.to_excel('categorized_keywords.xlsx', index=False)\n",
    "\n",
    "print(\"Data has been saved to categorized_keywords.xlsx\")\n",
    "\n",
    "# Print the DataFrame to verify\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find Sentence in Sentiment Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing subdirectory: BM_P/Ambitious\n",
      "Processing student directory: BM_P/Ambitious/2019-B-010\n",
      "\n",
      "Reading PDF file: BM_P/Ambitious/2019-B-010/讀書計畫.pdf\n",
      "Concatenated text for BM_P/Ambitious/2019-B-010:\n",
      "Sample extracted text from PDF ...\n",
      "Total length of concatenated text: 31\n",
      "Analyzed text for BM_P/Ambitious/2019-B-010:\n",
      "I'm sorry, I would need the actual text from the student's essay to identify specific sentences or parts that suggest the student is ambitious. Please provide the relevant content.\n",
      "\n",
      "Error processing GPT-4 output: Expecting value: line 1 column 2 (char 1)\n"
     ]
    }
   ],
   "source": [
    "base_dir = 'BM_P'\n",
    "sub_dirs = ['Ambitious']\n",
    "\n",
    "sentence_persona = \"You are an expert in analyzing student application for a universty. You have been asked to review a student's application essay\"\n",
    "\n",
    "def gpt_sentence_identification(essay_text):\n",
    "    # Prompt to send to GPT-4\n",
    "    prompt = f\"\"\"\n",
    "    Here is a student's essay:\n",
    "    \n",
    "    {essay_text}\n",
    "    \n",
    "    Identify three specific sentences or parts of the text that suggest the student is ambitious.\n",
    "\n",
    "    The output should be like this format and no other format is accepted:\n",
    "    {{\n",
    "        \"Sentence\": \"\"\n",
    "    }},\n",
    "    {{\n",
    "        \"Sentence\": \"\"\n",
    "    }},\n",
    "    {{\n",
    "        \"Sentence\": \"\"\n",
    "    }}\n",
    "    \"\"\"\n",
    "\n",
    "    response = openai.ChatCompletion.create(\n",
    "        model=\"gpt-4o\",\n",
    "        messages=[\n",
    "            {\n",
    "                \"role\": \"system\", \n",
    "                \"content\": sentence_persona\n",
    "            },\n",
    "            {\"role\": \"user\", \"content\": prompt},\n",
    "        ],\n",
    "    )\n",
    "    \n",
    "    assistant_reply = response['choices'][0]['message']['content']\n",
    "    return assistant_reply\n",
    "\n",
    "def process_gpt_output(assistant_reply):\n",
    "    try:\n",
    "        # Convert the string to JSON format\n",
    "        sentences_data = json.loads(f'[{assistant_reply.replace(\"},\", \"},\")}]')\n",
    "        # Extract just the sentences\n",
    "        sentences = [entry[\"Sentence\"] for entry in sentences_data]\n",
    "        return sentences\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing GPT-4 output: {e}\")\n",
    "        return []\n",
    "\n",
    "# main code\n",
    "data = []\n",
    "for sub_dir in sub_dirs:\n",
    "    sub_dir_path = os.path.join(base_dir, sub_dir)\n",
    "    print(f\"Processing subdirectory: {sub_dir_path}\")\n",
    "    \n",
    "    student_dirs = sorted(glob.glob(os.path.join(sub_dir_path, '2019-B-010')))\n",
    "    \n",
    "    if not student_dirs:\n",
    "        print(f\"No student directories found in {sub_dir_path}\\n\")\n",
    "        continue\n",
    "        \n",
    "    for student_dir in student_dirs:\n",
    "        print(f\"Processing student directory: {student_dir}\\n\")\n",
    "        \n",
    "        pdf_files = sorted(glob.glob(os.path.join(student_dir, '*.pdf')))\n",
    "        if not pdf_files:\n",
    "            print(f\"No PDF files found in {student_dir}\")\n",
    "            continue\n",
    "        \n",
    "        concatenated_text = \"\"\n",
    "        for pdf_file in pdf_files:\n",
    "            print(f\"Reading PDF file: {pdf_file}\")\n",
    "            pdf_text = extract_text_from_pdf(pdf_file)\n",
    "            concatenated_text += pdf_text + \" \"\n",
    "        \n",
    "        print(f\"Concatenated text for {student_dir}:\\n{concatenated_text[:1000]}...\")  \n",
    "        print(f\"Total length of concatenated text: {len(concatenated_text)}\")\n",
    "    \n",
    "\n",
    "    analyzed_text = gpt_sentence_identification(concatenated_text)\n",
    "    if analyzed_text:\n",
    "        print(f\"Analyzed text for {student_dir}:\\n{analyzed_text}\\n\")\n",
    "            \n",
    "        # Process the output and get the sentences\n",
    "        sentences = process_gpt_output(analyzed_text)\n",
    "            \n",
    "        # Add to data for further processing\n",
    "        data.append({\"Student Directory\": student_dir, \"Ambitious Sentences\": \"\\n\".join(sentences)})"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
