{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting Up the environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install PyPDF2\n",
    "%pip install pandas\n",
    "%pip install re\n",
    "%pip install openai==0.28\n",
    "%pip install openpyxl\n",
    "%pip install pdf2image pytesseract\n",
    "%pip install PyMuPDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "import glob\n",
    "from PyPDF2 import PdfReader\n",
    "import pandas as pd\n",
    "import re\n",
    "from pdf2image import convert_from_path\n",
    "import pytesseract\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "import fitz\n",
    "import json\n",
    "from openpyxl import Workbook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load OpenAI API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "api_key = os.getenv('OPENAI_API_KEY')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Cleaning: Extract Text from the PDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_text_extracted_meaningful(text):\n",
    "    # Check if the text is too short or seems not meaningful\n",
    "    return len(text.strip()) > 100  # Adjust the threshold as needed\n",
    "\n",
    "# Function to extract text from PDF\n",
    "def extract_text_from_pdf(pdf_path):\n",
    "    try:\n",
    "        # Try extracting text using PyMuPDF\n",
    "        text = \"\"\n",
    "        document = fitz.open(pdf_path)\n",
    "        for page_num in range(len(document)):\n",
    "            page = document.load_page(page_num)\n",
    "            extracted_text = page.get_text(\"text\")\n",
    "            if extracted_text:\n",
    "                text += extracted_text\n",
    "\n",
    "        # Fallback to OCR if text extraction is not meaningful\n",
    "        if not is_text_extracted_meaningful(text):\n",
    "            text = \"\"\n",
    "            images = convert_from_path(pdf_path)\n",
    "            for image in images:\n",
    "                text += pytesseract.image_to_string(image)\n",
    "        \n",
    "        text = text.replace(\"\\n\", \" \").replace(\"\\t\", \" \")\n",
    "        return text\n",
    "    except Exception as e:\n",
    "        print(f\"Error occurred: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GPT 4o"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Personality Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "personality_persona = \"You are an expert in analyzing student application for a universty.You have been asked to review a student's application essay and provide an analysis on the student's motivation.\"\n",
    "personality_prompt = f\"\"\"\n",
    "Please analyze the following student's application statement to identify 10 keywords that indicate the student's personality. Provide an explanation of the student personality based on the keyword. For each keyword, provide the following information in the specified format:\n",
    "\n",
    "[\n",
    "  {{\n",
    "    \"Keyword\": \"\",\n",
    "    \"Explanation\": \"\",\n",
    "    \"Sentence Related to Keyword\": \"\"\n",
    "  }}\n",
    "]\n",
    "\n",
    "The output should be in this JSON format and no other output is accepted.\n",
    "\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Motivation Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gpt_input(statement,persona,prompt):\n",
    "    response = openai.ChatCompletion.create(\n",
    "        model=\"gpt-4o\",  # Use the appropriate model name\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\":f\"{persona} {prompt}\"},\n",
    "            {\"role\": \"user\", \"content\": statement},\n",
    "        ]\n",
    "    )\n",
    "\n",
    " # Extract the assistant's reply\n",
    "    assistant_reply = response['choices'][0]['message']['content']\n",
    "    return assistant_reply"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parsing the Output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Personality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_personality_text(analyzed_text):\n",
    "    lines = analyzed_text.split('\\n')\n",
    "    parsed_data = []\n",
    "    current_keyword = \"\"\n",
    "    current_explanation = \"\"\n",
    "    \n",
    "    # Regular expression patterns to match different keyword formats\n",
    "    pattern1 = re.compile(r'\\d+\\.\\s*\\*\\*(.*?)\\*\\*')\n",
    "    pattern2 = re.compile(r'\\*\\*\\d+\\.\\s*(.*?):\\*\\*')\n",
    "    \n",
    "    for line in lines:\n",
    "        match1 = pattern1.match(line)\n",
    "        match2 = pattern2.match(line)\n",
    "        if match1:\n",
    "            if current_keyword and current_explanation:\n",
    "                parsed_data.append([current_keyword, current_explanation.strip()])\n",
    "            current_keyword = match1.group(1).strip()\n",
    "            current_explanation = line[match1.end():].strip()  # Start explanation from the rest of the line if present\n",
    "        elif match2:\n",
    "            if current_keyword and current_explanation:\n",
    "                parsed_data.append([current_keyword, current_explanation.strip()])\n",
    "            current_keyword = match2.group(1).strip()\n",
    "            current_explanation = line[match2.end():].strip()  # Start explanation from the rest of the line if present\n",
    "        else:\n",
    "            if current_keyword:\n",
    "                if line.strip():  # Only add lines that are not just empty or whitespace\n",
    "                    current_explanation += ' ' + line.strip()\n",
    "    \n",
    "    # Add the last keyword and explanation if present\n",
    "    if current_keyword and current_explanation:\n",
    "        parsed_data.append([current_keyword, current_explanation.strip()])\n",
    "    \n",
    "    return parsed_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Motivation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def parse_motivation_text(analyzed_text):\n",
    "#     if not analyzed_text or not analyzed_text.strip():\n",
    "#             print(\"The input text is empty or None.\")\n",
    "#             return None\n",
    "        \n",
    "#     try:\n",
    "#         data = json.loads(analyzed_text)\n",
    "#     except json.JSONDecodeError as e:\n",
    "#         print(f\"JSON decoding error: {e}\")\n",
    "#         return None\n",
    "    \n",
    "#     return data\n",
    "\n",
    "def parse_motivation_text(analyzed_text):\n",
    "    # Check if the input text is empty or None\n",
    "    if not analyzed_text or not analyzed_text.strip():\n",
    "        print(\"The input text is empty or None.\")\n",
    "        return None\n",
    "    \n",
    "    # Check for and remove the JSON heading if present\n",
    "    if analyzed_text.startswith(\"```json\"):\n",
    "        analyzed_text = analyzed_text.split('\\n', 1)[1]\n",
    "    \n",
    "    try:\n",
    "        # Attempt to decode the JSON\n",
    "        data = json.loads(analyzed_text)\n",
    "    except json.JSONDecodeError as e:\n",
    "        print(f\"JSON decoding error: {e}\")\n",
    "        return None\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save Output to Excel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_to_excel(parsed_data, ID, output_file):\n",
    "    # # Create a DataFrame from the parsed data\n",
    "    # df = pd.DataFrame(parsed_data, columns=['Personality', 'Explanation'])\n",
    "    \n",
    "    # # Add a column for the file name\n",
    "    # df.insert(0, 'File Name', ID)\n",
    "    \n",
    "    # if os.path.exists(output_file):\n",
    "    #     # If the file exists, read the existing data\n",
    "    #     existing_df = pd.read_excel(output_file)\n",
    "    #     # Append the new data\n",
    "    #     df = pd.concat([existing_df, df], ignore_index=True)\n",
    "    \n",
    "    # # Save the updated DataFrame back to the Excel file\n",
    "    # df.to_excel(output_file, index=False)\n",
    "    # print(f\"Results have been saved to {output_file}\")\n",
    "     # Create a DataFrame from the parsed data\n",
    "    # Check if parsed_data is valid\n",
    "    if parsed_data is None:\n",
    "        print(\"No data to save.\")\n",
    "        return\n",
    "    \n",
    "    # Create a DataFrame from the parsed data\n",
    "    df = pd.DataFrame(parsed_data, columns=['Keyword', 'Explanation', 'Sentence Related to Keyword'])\n",
    "    \n",
    "    # Add a column for the file name\n",
    "    df.insert(0, 'File Name', ID)\n",
    "    \n",
    "    if os.path.exists(output_file):\n",
    "        # If the file exists, read the existing data\n",
    "        existing_df = pd.read_excel(output_file)\n",
    "        # Append the new data\n",
    "        df = pd.concat([existing_df, df], ignore_index=True)\n",
    "    \n",
    "    # Save the updated DataFrame back to the Excel file\n",
    "    df.to_excel(output_file, index=False, engine='openpyxl')\n",
    "    print(f\"Results have been saved to {output_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Main Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "motivation_persona = \"You are an expert in analyzing student applications for a university. You have been asked to review a student's application essay and provide an analysis on the student's motivation.\"\n",
    "motivation_prompt = f\"\"\"\n",
    "Please analyze the following student's application statement to identify 10 keywords that indicate their motivation to study in university. For each keyword, provide the following information in the specified format:\n",
    "[\n",
    "  {{\n",
    "    \"Keyword\": \"\",\n",
    "    \"Explanation\": \"\",\n",
    "    \"Sentence Related to Keyword\": \"\"\n",
    "  }}\n",
    "]\n",
    "The output should be in this format and no other output is accepted.\n",
    "\"\"\"\n",
    "# personality_persona = \"You are an expert in analyzing student application for a universty.You have been asked to review a student's application essay and provide an analysis on the student's motivation.\"\n",
    "# personality_prompt = f\"\"\"\n",
    "# Please analyze the following student's application statement to identify 10 keywords that indicate the student's personality. Provide an explanation of the student personality based on the keyword. For each keyword, provide the following information in the specified format:\n",
    "\n",
    "# [\n",
    "#   {{\n",
    "#     \"Keyword\": \"\",\n",
    "#     \"Explanation\": \"\",\n",
    "#     \"Sentence Related to Keyword\": \"\"\n",
    "#   }}\n",
    "# ]\n",
    "\n",
    "# The output should be in this format and no other output is accepted.\n",
    "\n",
    "# \"\"\"\n",
    "\n",
    "base_dir = 'temp'\n",
    "sub_dirs = ['c-enrolled']\n",
    "output_file = 'temp_22c.xlsx'\n",
    "\n",
    "for sub_dir in sub_dirs:\n",
    "    sub_dir_path = os.path.join(base_dir, sub_dir)\n",
    "    print(f\"Processing subdirectory: {sub_dir_path}\")\n",
    "    \n",
    "    student_dirs = sorted(glob.glob(os.path.join(sub_dir_path, '2021-A-*')))\n",
    "    \n",
    "    if not student_dirs:\n",
    "        print(f\"No student directories found in {sub_dir_path}\\n\")\n",
    "        continue\n",
    "        \n",
    "    for student_dir in student_dirs:\n",
    "        print(f\"Processing student directory: {student_dir}\\n\")\n",
    "        \n",
    "        pdf_files = sorted(glob.glob(os.path.join(student_dir, '*.pdf')))\n",
    "        if not pdf_files:\n",
    "            print(f\"No PDF files found in {student_dir}\")\n",
    "            continue\n",
    "        \n",
    "        concatenated_text = \"\"\n",
    "        for pdf_file in pdf_files:\n",
    "            print(f\"Reading PDF file: {pdf_file}\")\n",
    "            pdf_text = extract_text_from_pdf(pdf_file)\n",
    "            concatenated_text += pdf_text + \" \"\n",
    "        \n",
    "        print(f\"Concatenated text for {student_dir}:\\n{concatenated_text[:1000]}...\")  \n",
    "        print(f\"Total length of concatenated text: {len(concatenated_text)}\")\n",
    "        \n",
    "        analyzed_text = gpt_input(concatenated_text, motivation_persona, motivation_prompt)\n",
    "        print(f\"Analyzed text for {student_dir}:\\n{analyzed_text}\\n\")\n",
    "        parsed_data = parse_motivation_text(analyzed_text)\n",
    "        print(f\"Parsed data for {student_dir}:\\n{parsed_data}\\n\")\n",
    "        student_dir_name = os.path.basename(student_dir)\n",
    "        save_to_excel(parsed_data, student_dir_name, output_file)\n",
    "\n",
    "print(f\"Results have been saved to {output_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Motivation Category Analysis by GPT API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = 'Statistics_2022eecs_GPT.xlsx'\n",
    "\n",
    "df = pd.read_excel(file_path, sheet_name='ENROLLED_MOTIVATION')  # Replace 'Sheet1' with your sheet name\n",
    "# Extract unique values from a specific column (e.g., 'Column1')\n",
    "unique_values = df['Keyword'].unique()\n",
    "\n",
    "# Create a new DataFrame for the unique values\n",
    "unique_df = pd.DataFrame(unique_values, columns=['Unique Values'])\n",
    "unique_list = unique_df.values.flatten().tolist()\n",
    "print(unique_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gave ChatGPT Persona"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "persona = \"\"\"\n",
    "You are a data analysis specializing in natural language processing and keyword extraction.\n",
    "You will be given a list of keywords about student motivation to study in university, reorganize the following keywords into appropriate categories.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gpt_personality_conclusion(statement,persona):\n",
    "    response = openai.ChatCompletion.create(\n",
    "        model=\"gpt-4o\",  # Use the appropriate model name\n",
    "        messages=[\n",
    "            {\n",
    "                \"role\": \"system\", \n",
    "                \"content\": persona\n",
    "            },\n",
    "            {\"role\": \"user\", \"content\": statement},\n",
    "        ],\n",
    "    )\n",
    "\n",
    "    # Extract the assistant's reply\n",
    "    assistant_reply = response['choices'][0]['message']['content']\n",
    "    return assistant_reply"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Find the top keywords from all the keywords in excel\n",
    "keywords_list = unique_list\n",
    "\n",
    "keywords_prompt = f\"\"\"{persona}\\n. Here is your task:\\n\n",
    "                    1. Read all of the Keywords.\\n\n",
    "                    2. Take the keyword input and recategorized this keyword into 20 category.\\n\n",
    "                    2. If they mention school name such as NTHU, make the category School.\\n\n",
    "                    3. Make sure all keyword got into category.\\n\n",
    "                    4. Make the category more specific. No miscellaneous category.\\n\n",
    "                    5. You should output the data in the following format:\\n\\n\n",
    "                        1. [Category]: Keyword, Keyword, Keyword,..\\n\n",
    "                    The output should be in this format and no other output is accepted.\\n\n",
    "                    Here is the list of keywords that shows student motivation to study in university: {keywords_list}\\n\n",
    "                    \"\"\"\n",
    "keywords_category = gpt_personality_conclusion(keywords_prompt,persona)\n",
    "print(\"Category and Keyword:\")\n",
    "print(keywords_category)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use regular expressions to extract keyword categories and keywords\n",
    "pattern = re.compile(r'\\d+\\.\\s+([^:]+):\\s+(.+)')\n",
    "matches = pattern.findall(keywords_category)\n",
    "\n",
    "# Prepare data for the DataFrame\n",
    "data = []\n",
    "for category, keywords in matches:\n",
    "    keywords_list = [k.strip() for k in keywords.split(',')]\n",
    "    for keyword in keywords_list:\n",
    "        data.append({'keyword category': category.strip(), 'keyword': keyword})\n",
    "\n",
    "# Convert the data to a DataFrame\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Save the DataFrame to an Excel file\n",
    "df.to_excel('categorized_keywords.xlsx', index=False)\n",
    "\n",
    "print(\"Data has been saved to categorized_keywords.xlsx\")\n",
    "\n",
    "# Print the DataFrame to verify\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Find Sentence in Sentiment Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing subdirectory: ./AI_Sentence/BM-P/Ambitious\n",
      "Processing student directory: ./AI_Sentence/BM-P/Ambitious/2022-BM-000\n",
      "\n",
      "Reading PDF file: ./AI_Sentence/BM-P/Ambitious/2022-BM-000/2022-BM-000-Personal Statement.pdf\n",
      "Reading PDF file: ./AI_Sentence/BM-P/Ambitious/2022-BM-000/2022-BM-000-Statement of Purpose.pdf\n",
      "Concatenated text for ./AI_Sentence/BM-P/Ambitious/2022-BM-000:\n",
      "  Personal Statement        Throughout 17 years of my life, I have always been curious about how one business  can grow or what brings the best out of it, including how they convey it through messages.  Thanks to my mom’s business and my motivation, the business has become a part of my  life. At the age of ten, I decided to start a small online business, cultivating a brand from  nothing. It provided me with the opportunity to learn and manage my ﬁnances, as well as  basic marketing strategies such as Google Ads targeting and a Facebook page. As I  wanted to know more about it, I enrolled in a ﬁve-week management course on Coursera,  and it has changed my whole perception of brand purpose when it becomes the factor that  drives an entire organization and ties the brand to the customers.    My full name is Jareonrat Chinpathirunkul; my family and friends call me by my  nickname, Yok. I am currently a student of Chinese language major at a very competitive  Thai high school which I took ...\n",
      "Total length of concatenated text: 7190\n",
      "Analyzed text for ./AI_Sentence/BM-P/Ambitious/2022-BM-000:\n",
      "\"Sentence\": \"Having my own online business and investing in an international stock market is my dream career.\"\n",
      "\n",
      "Collected data for 2022-BM-000\n",
      "\n",
      "Processing student directory: ./AI_Sentence/BM-P/Ambitious/2022-BM-030\n",
      "\n",
      "Reading PDF file: ./AI_Sentence/BM-P/Ambitious/2022-BM-030/2022-BM-030-Φç¬σé│.pdf\n",
      "Reading PDF file: ./AI_Sentence/BM-P/Ambitious/2022-BM-030/2022-BM-030-Φ«Çµ¢╕Φ¿êσèâ.pdf\n",
      "Concatenated text for ./AI_Sentence/BM-P/Ambitious/2022-BM-030:\n",
      "Personal Statement My name is Tran Khoa Linh, I will be graduating from Nguyen Tat Thanh high school, where I excelled to be one of the top students among my peers, in July, 2022. I see myself as a determined person, who longs to accomplish my goals. Also, I am a problem solver, I always seek solutions to tackle or resolve problems and I like to confront challenges, which can build my resilience to overcome obstacles whenever I fail or succeed. My decision to pursue a Bachelor’s degree in Business Management since I found out that today’s world is a business-oriented economy. Specifically, our daily lives and other fields are business-related and comprehending business will be the best interest. To begin with, being raised in Vietnam has sparked my interest in business topics, such as E-commerce, consumer markets, and how Vietnam has captivated a large number of foreign investors, which prompted me to approach some business news. I discovered that competitive labor costs, ample workfor...\n",
      "Total length of concatenated text: 13864\n",
      "Analyzed text for ./AI_Sentence/BM-P/Ambitious/2022-BM-030:\n",
      "\"Sentence\": \"Lastly, as an ambitious and a top student of my school, I am certain that I would bring my competence, ambition and my enthusiasm to my classmates as well as professors at the university.\"\n",
      "\n",
      "Collected data for 2022-BM-030\n",
      "\n",
      "All data written to ambitious_sentences.xlsx\n",
      "\n"
     ]
    }
   ],
   "source": [
    "base_dir = './AI_Sentence/BM-P'\n",
    "sub_dirs = ['Ambitious']\n",
    "\n",
    "sentence_persona = \"You are an expert in analyzing student application for a university. You have been asked to review a student's application essay.\"\n",
    "\n",
    "def gpt_sentence_identification(essay_text):\n",
    "    # Prompt to send to GPT-4\n",
    "    prompt = f\"\"\"\n",
    "    Here is a student's essay:\n",
    "    \n",
    "    {essay_text}\n",
    "    \n",
    "    Identify one specific sentence or part of the text that suggests the student is ambitious.\n",
    "\n",
    "    The output should be like this format and no other format is accepted:\n",
    "        \"Sentence\": \"\"\n",
    "    \"\"\"\n",
    "\n",
    "    response = openai.ChatCompletion.create(\n",
    "        model=\"gpt-4o\",\n",
    "        messages=[\n",
    "            {\n",
    "                \"role\": \"system\", \n",
    "                \"content\": sentence_persona\n",
    "            },\n",
    "            {\"role\": \"user\", \"content\": prompt},\n",
    "        ],\n",
    "    )\n",
    "    \n",
    "    assistant_reply = response['choices'][0]['message']['content']\n",
    "    return assistant_reply\n",
    "\n",
    "def process_gpt_output(assistant_reply):\n",
    "    # Process plain text GPT output\n",
    "    try:\n",
    "        # Extract the sentence part from the output\n",
    "        start = assistant_reply.find('\"Sentence\":')\n",
    "        if start == -1:\n",
    "            print(\"Error: 'Sentence:' not found in the reply.\")\n",
    "            return \"\"\n",
    "        \n",
    "        # Extract the sentence\n",
    "        start += len('\"Sentence\":')  # Move the pointer after 'Sentence:'\n",
    "        sentence = assistant_reply[start:].strip()\n",
    "\n",
    "        # Remove quotation marks if present\n",
    "        if sentence.startswith('\"') and sentence.endswith('\"'):\n",
    "            sentence = sentence[1:-1]\n",
    "\n",
    "        return sentence\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing GPT-4 output: {e}\")\n",
    "        return \"\"\n",
    "\n",
    "excel_filename = \"ambitious_sentences.xlsx\"\n",
    "columns = [\"Student ID\", \"Sentence\"]\n",
    "\n",
    "# Create a new Excel file or load it if it already exists\n",
    "if not os.path.exists(excel_filename):\n",
    "    df = pd.DataFrame(columns=columns)\n",
    "    df.to_excel(excel_filename, index=False)\n",
    "\n",
    "# main code\n",
    "data = []  # List to hold the data to be written to Excel at the end\n",
    "for sub_dir in sub_dirs:\n",
    "    sub_dir_path = os.path.join(base_dir, sub_dir)\n",
    "    print(f\"Processing subdirectory: {sub_dir_path}\")\n",
    "    \n",
    "    student_dirs = sorted(glob.glob(os.path.join(sub_dir_path, '2022-BM-*')))\n",
    "    \n",
    "    if not student_dirs:\n",
    "        print(f\"No student directories found in {sub_dir_path}\\n\")\n",
    "        continue\n",
    "        \n",
    "    for student_dir in student_dirs:\n",
    "        student_id = os.path.basename(student_dir)\n",
    "        print(f\"Processing student directory: {student_dir}\\n\")\n",
    "        \n",
    "        pdf_files = sorted(glob.glob(os.path.join(student_dir, '*.pdf')))\n",
    "        if not pdf_files:\n",
    "            print(f\"No PDF files found in {student_dir}\")\n",
    "            continue\n",
    "        \n",
    "        concatenated_text = \"\"\n",
    "        for pdf_file in pdf_files:\n",
    "            print(f\"Reading PDF file: {pdf_file}\")\n",
    "            pdf_text = extract_text_from_pdf(pdf_file)\n",
    "            concatenated_text += pdf_text + \" \"\n",
    "        \n",
    "        print(f\"Concatenated text for {student_dir}:\\n{concatenated_text[:1000]}...\")  \n",
    "        print(f\"Total length of concatenated text: {len(concatenated_text)}\")\n",
    "    \n",
    "\n",
    "        analyzed_text = gpt_sentence_identification(concatenated_text)\n",
    "        print(f\"Analyzed text for {student_dir}:\\n{analyzed_text}\\n\")\n",
    "        \n",
    "        output_sentence = process_gpt_output(analyzed_text)\n",
    "        \n",
    "        if output_sentence:\n",
    "            # Append the result to the data list\n",
    "            new_data = {\"Student ID\": student_id, \"Sentence\": output_sentence}\n",
    "            data.append(new_data)\n",
    "            print(f\"Collected data for {student_id}\\n\")\n",
    "\n",
    "# Convert data to a DataFrame and append to the Excel file\n",
    "if data:\n",
    "    df = pd.read_excel(excel_filename)\n",
    "    new_df = pd.DataFrame(data)\n",
    "    df = pd.concat([df, new_df], ignore_index=True)\n",
    "    df.to_excel(excel_filename, index=False)\n",
    "    print(f\"All data written to {excel_filename}\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
